{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7sLkxfLuR-l"
      },
      "source": [
        "\"\"\"VLP_classification.ipynb\n",
        "\n",
        "Automatically generated by Colab.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/19yn-ef-FixANlQFJTMEwD9GPb_9jkKPt\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jYwMUuZ1v_L"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir(\"drive/MyDrive/VLP_classify\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmhW2VTM3j1i"
      },
      "outputs": [],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kImcefEYNkgE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "data =  pd.read_csv(\"testdata.csv\", header=None)\n",
        "statis = []\n",
        "for index, row in data.iterrows():\n",
        "  count=0\n",
        "  for col in row:\n",
        "    if not np.isnan(col):\n",
        "      count+=1\n",
        "  statis.append((index,count))\n",
        "print(statis)\n",
        "sorted_list = sorted(statis, key=lambda t: t[1])\n",
        "print(sorted_list) # sort data by row length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlSEavC-_nFu"
      },
      "outputs": [],
      "source": [
        "len(sorted_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikpgkxRb_vJk"
      },
      "outputs": [],
      "source": [
        "sorted_list[200]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugD4PD2Y1pX9"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(12, 6))\n",
        "bar = [t[1] for t in sorted_list]\n",
        "plt.bar(range(len(bar)), bar)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bNF1emSLD5ae"
      },
      "outputs": [],
      "source": [
        "index = [t[0] for t in sorted_list]\n",
        "print(index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TmoBXjDlEk68"
      },
      "outputs": [],
      "source": [
        "temp = data.iloc[535]\n",
        "temp = list(i for i in temp if not np.isnan(i))\n",
        "print(len(temp))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4MVHHFH6Yt4"
      },
      "outputs": [],
      "source": [
        "resample_data = []\n",
        "label = []\n",
        "for i in index:\n",
        "  temp = []\n",
        "  for ind, val in enumerate(data.iloc[i]):\n",
        "    if ind == 0:\n",
        "      label.append(val)\n",
        "    elif not np.isnan(val):\n",
        "      temp.append(val)\n",
        "  resample_data.append(pd.Series(temp))\n",
        "print(len(resample_data))\n",
        "print(len(label))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "daG9fzhbOvVz"
      },
      "outputs": [],
      "source": [
        "resample_data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Lx7MOaiXolj"
      },
      "outputs": [],
      "source": [
        "test = resample_data[0]\n",
        "new_index = pd.Index(np.linspace(0, len(test) - 1, num=1000))\n",
        "#print(new_index)\n",
        "interpolated_sequence = pd.Series(np.interp(new_index, test.index, test.values), index=new_index)\n",
        "\n",
        "# The sequence after supplementing to the target length\n",
        "print(interpolated_sequence.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQgoiYaUhmDO"
      },
      "outputs": [],
      "source": [
        "cleaned_data = []\n",
        "target_length = 500\n",
        "for seq in resample_data:\n",
        "  if len(seq)<=target_length:\n",
        "    # make up length\n",
        "    new_index = pd.Index(np.linspace(0, len(seq)-1, num=target_length))\n",
        "    new_seq = np.interp(new_index, seq.index, seq.values)\n",
        "    cleaned_data.append(new_seq)\n",
        "  else:\n",
        "    num = len(seq)\n",
        "    new_index = pd.Index(np.linspace(0, len(seq)-1, num=target_length))\n",
        "    new_seq = np.interp(new_index, seq.index, seq.values)\n",
        "    cleaned_data.append(new_seq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I03nGFxe_Umj"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "bar = [len(t) for t in cleaned_data]\n",
        "plt.bar(range(len(bar)), bar)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eqvtFtmF_gR8"
      },
      "outputs": [],
      "source": [
        "for index, values in enumerate(cleaned_data):\n",
        "  plt.plot(range(500), values)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQzW9Siy7yLo"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "X_embedded = TSNE(n_components=2, learning_rate='auto', init='random', perplexity=5).fit_transform(np.asarray(cleaned_data))\n",
        "X_embedded.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWVNl2Ug78v6"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(8,5))\n",
        "for i in range(2):\n",
        "  print(i)\n",
        "  x = []\n",
        "  print(type(x))\n",
        "  y = []\n",
        "  for index, j in enumerate(label):\n",
        "    if j == i:\n",
        "      x.append(X_embedded[index][0])\n",
        "      y.append(X_embedded[index][1])\n",
        "  plt.scatter(x, y)\n",
        "\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6s_lIjfDBvm"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ee9R2ypIfa3D"
      },
      "outputs": [],
      "source": [
        "# prepare train data and label\n",
        "data_sequences = np.array(cleaned_data)\n",
        "label_encoder = LabelEncoder()\n",
        "data_labels = label_encoder.fit_transform(label)  # labels for training data\n",
        "# print(data_labels)\n",
        "\n",
        "# # calculate 1-Norm bumber for each line\n",
        "# norms = np.linalg.norm(data_sequences, axis=1, keepdims=True)\n",
        "# # standardization for each line\n",
        "# data_sequences = data_sequences / norms\n",
        "\n",
        "# seperate the train and test dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(data_sequences, data_labels, test_size=0.2, random_state = 7)\n",
        "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
        "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        \"best_model.keras\", save_best_only=True, monitor=\"binary_accuracy\", mode='max'\n",
        "    ),\n",
        "    keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor=\"val_loss\", factor=0.5, patience=20, min_lr=0.0001\n",
        "    ),\n",
        "    keras.callbacks.EarlyStopping(monitor=\"binary_accuracy\", mode='max', patience=50, verbose=1),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sv8QoME3A67Z"
      },
      "outputs": [],
      "source": [
        "print(len([x for x in y_train if x==0])/ len(y_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "foIP4QBFH_Ck"
      },
      "outputs": [],
      "source": [
        "# CNN\n",
        "regularizer = 0\n",
        "input = keras.layers.Input(shape=(X_train.shape[1],1))\n",
        "# x = keras.layers.Reshape((X_train.shape[1], 1))(input)\n",
        "x = keras.layers.Conv1D(32, 10, activation='relu',kernel_regularizer=keras.regularizers.l2(regularizer))(input)\n",
        "x = keras.layers.BatchNormalization()(x)\n",
        "x = keras.layers.ReLU()(x)\n",
        "\n",
        "x = keras.layers.Conv1D(64, 20, activation='relu')(x)\n",
        "x = keras.layers.BatchNormalization()(x)\n",
        "x = keras.layers.ReLU()(x)\n",
        "\n",
        "x = keras.layers.Conv1D(64, 10, activation='relu',kernel_regularizer=keras.regularizers.l2(regularizer))(x)\n",
        "x = keras.layers.GlobalAvgPool1D()(x)\n",
        "x = keras.layers.Flatten()(x)\n",
        "x = keras.layers.Dense(10, activation='relu')(x)\n",
        "output = keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "mymodel = keras.Model(input, output)\n",
        "mymodel.compile(\n",
        "    loss=keras.losses.BinaryCrossentropy(),\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "    metrics=[keras.metrics.BinaryAccuracy()]\n",
        ")\n",
        "mymodel.summary()\n",
        "\n",
        "mymodel.fit(X_train,\n",
        "            y_train,\n",
        "            epochs=500,\n",
        "            batch_size=4,\n",
        "            validation_data=(X_test, y_test),\n",
        "            callbacks=callbacks,\n",
        "            verbose=1,\n",
        "            )\n",
        "\n",
        "model = keras.models.load_model(\"best_model.keras\")\n",
        "model.evaluate(X_test,y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYnULGVQK8Eb"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "mymodel = keras.models.load_model(\"best_model_0910.keras\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJvqFzUgkv6p"
      },
      "outputs": [],
      "source": [
        "def create_truncated_model(trained_model):\n",
        "  regularizer = 0\n",
        "  input = keras.layers.Input(shape=(X_train.shape[1],1))\n",
        "  x = keras.layers.Conv1D(32, 10, activation='relu',kernel_regularizer=keras.regularizers.l2(regularizer))(input)\n",
        "  x = keras.layers.BatchNormalization()(x)\n",
        "  x = keras.layers.ReLU()(x)\n",
        "\n",
        "  x = keras.layers.Conv1D(64, 20, activation='relu')(x)\n",
        "  x = keras.layers.BatchNormalization()(x)\n",
        "  x = keras.layers.ReLU()(x)\n",
        "\n",
        "  x = keras.layers.Conv1D(64, 10, activation='relu',kernel_regularizer=keras.regularizers.l2(regularizer))(x)\n",
        "  x = keras.layers.GlobalAvgPool1D()(x)\n",
        "  x = keras.layers.Flatten()(x)\n",
        "  output = keras.layers.Dense(10, activation='relu')(x)\n",
        "  new_model = keras.Model(input, output)\n",
        "\n",
        "  for i, layer in enumerate(new_model.layers):\n",
        "    layer.set_weights(trained_model.layers[i].get_weights())\n",
        "\n",
        "  new_model.compile(\n",
        "    loss=keras.losses.BinaryCrossentropy(),\n",
        "    optimizer=keras.optimizers.RMSprop(learning_rate=0.0005),\n",
        "    metrics=[keras.metrics.BinaryAccuracy()]\n",
        "  )\n",
        "\n",
        "  return new_model\n",
        "\n",
        "truncated_model = create_truncated_model(mymodel)\n",
        "hidden_features = truncated_model.predict(X_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbyuRt0gBJE7"
      },
      "source": [
        "Q393L+WT : 0;blue\n",
        "L53D+V108D : 1;orange"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kI5xaIxPobce"
      },
      "outputs": [],
      "source": [
        "X_embedded = TSNE(n_components=2, learning_rate='auto', init='random', perplexity=50).fit_transform(np.asarray(hidden_features))\n",
        "fig = plt.figure(figsize=(10,10),dpi=300)\n",
        "for i in range(2):\n",
        "  indices = np.where(y_train==i)\n",
        "  plt.scatter(X_embedded[indices, 0], X_embedded[indices, 1], label=str(i))\n",
        "#plt.legend()\n",
        "plt.gca().spines[['top','bottom','left','right']].set_linewidth(3)\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "# plt.xlabel(\"Dimension 1\", size=10)\n",
        "# plt.ylabel(\"Dimension 2\", size=10)\n",
        "plt.legend(['WT, Q393L','L53D, V108D'], fontsize=\"30\", markerscale=3)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
